{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regresion_lineal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE27xx7tUtP3"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBbXinYIVUJF"
      },
      "source": [
        "model that predicts crop yields for apples and oranges (*target variables*) by looking at the average temperature, rainfall, and humidity (*input variables or features*) in a region. Here's the training data:\n",
        "\n",
        "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
        "\n",
        "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
        "\n",
        "```\n",
        "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
        "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-EkhIn4VYkK"
      },
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70]], dtype='float32')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpmMiK8rVlis"
      },
      "source": [
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kArNjW4DV1gy"
      },
      "source": [
        "X and Y to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgnlG3e1VvBZ",
        "outputId": "9d30b56b-dbb2-4cc0-f52e-233f5eef5ae6"
      },
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUz0tzsEV8r8"
      },
      "source": [
        "The weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJde0SsaVwTI",
        "outputId": "81079c26-1725-49e9-fb87-35af736ddf70"
      },
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6295,  0.8457,  0.1010],\n",
            "        [ 0.8688,  1.4919, -0.2712]], requires_grad=True)\n",
            "tensor([-0.5647,  0.8586], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJCW-G17WSuM"
      },
      "source": [
        "`torch.randn` creates a tensor with the given shape, with elements picked randomly from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean 0 and standard deviation 1.\n",
        "\n",
        "Our *model* is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
        "\n",
        "![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n",
        "\n",
        "We can define the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a44256vVxP6"
      },
      "source": [
        "def model(x):\n",
        "    return x @ w.t() + b"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCTGv_6AWWnN"
      },
      "source": [
        "Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REkowItQWarF",
        "outputId": "f957795b-0969-4acc-9ac8-d9de6bcc1e06"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 14.4888, 152.5786],\n",
            "        [ 23.0391, 193.8525],\n",
            "        [ 63.8551, 260.6313],\n",
            "        [-24.6704, 143.5958],\n",
            "        [ 44.2600, 185.0468]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIGnjir2Wh6_"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ks5GwCWcFC"
      },
      "source": [
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykNqDOEeWcCt",
        "outputId": "b3e512bd-6028-410c-decb-07ff7dff30e1"
      },
      "source": [
        "# Compute loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6120.7529, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl-s1f_Avltp"
      },
      "source": [
        "## Compute gradients\n",
        "\n",
        "With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcBUFpFIWb-c"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Jc-7Yfv1Ga"
      },
      "source": [
        "The gradients are stored in the `.grad` property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix with the same dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_UP6MeIWb8n",
        "outputId": "8fa33645-4d6d-4e55-e259-ac0ae5b9ecc2"
      },
      "source": [
        "# Gradients for weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6295,  0.8457,  0.1010],\n",
            "        [ 0.8688,  1.4919, -0.2712]], requires_grad=True)\n",
            "tensor([[-4383.1611, -4583.4194, -2906.2979],\n",
            "        [ 8202.3477,  8346.0996,  5092.6758]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN5zCyhwRDn"
      },
      "source": [
        "\n",
        "## Adjust weights and biases to reduce the loss\n",
        "\n",
        "The loss is a [quadratic function](https://en.wikipedia.org/wiki/Quadratic_function) of our weights and biases, and our objective is to find the set of weights where the loss is the lowest\n",
        "\n",
        "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of _the gradient descent_ optimization algorithm that we'll use to improve our model (by _descending_ along the _gradient_).\n",
        "\n",
        "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKbBluNIWb1a",
        "outputId": "a571a987-e67f-4c57-d7bc-b18c4b7b79ca"
      },
      "source": [
        "w.grad"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4383.1611, -4583.4194, -2906.2979],\n",
              "        [ 8202.3477,  8346.0996,  5092.6758]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG_0wdOSWbxv"
      },
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Zxm_SR1DI5"
      },
      "source": [
        "We multiply the gradients with a very small number (10^-5 in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the learning rate of the algorithm.\n",
        "\n",
        "We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBqnrRWJWbtk",
        "outputId": "1d729c4c-f29e-4abd-b6de-b8a45096c1ed"
      },
      "source": [
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6120.7529, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_jfRBJD198s"
      },
      "source": [
        "Before we proceed, we reset the gradients to zero by invoking the `.zero_()` method. We need to do this because PyTorch accumulates gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug6FSyiZ1RA7",
        "outputId": "260303c3-7eec-43c4-d4f0-02f83a567739"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjlLLce2Ypb"
      },
      "source": [
        "## Train the model using gradient descent\n",
        "\n",
        "we can _train_ the model using the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero\n",
        "\n",
        "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an _epoch_. Let's train the model for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT5lZtDpWbk5"
      },
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(100):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zF-D-RdWbZd",
        "outputId": "255d234b-4c13-47af-bfc6-aa00766bb317"
      },
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(130.6653, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UdQGurGWbJ4",
        "outputId": "5475b03c-637c-449d-e471-e3165a995d5f"
      },
      "source": [
        "# Predictions\n",
        "preds"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.8944,  73.7796],\n",
              "        [ 78.8448,  92.4798],\n",
              "        [126.7730, 146.0092],\n",
              "        [ 19.7341,  55.2962],\n",
              "        [ 96.7469,  94.3893]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMBuf2mN3du3",
        "outputId": "7c04d45a-5630-449e-d247-6bcabdc6ceaa"
      },
      "source": [
        "# Targets\n",
        "targets"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}